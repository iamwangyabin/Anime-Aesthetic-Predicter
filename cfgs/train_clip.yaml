name: "clip"  # Name of the experiment for organizing samples and models

train:
  gpu_ids: [0,1]  # GPU IDs to use for training, e.g., '0' for single GPU or '0,1,2,3' for multiple GPUs
  train_epochs: 20  # Number of training epochs
  gradient_accumulation_steps: 1
  check_val_every_n_epoch: 1

  optimizer:
    _target_: torch.optim.AdamW
    _partial_: True
    lr: 1e-4
    weight_decay: 1e-3
 
  scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    _partial_: True
    T_max: $train.train_epochs

dataset:
  dataroot: "/data/jwang/aesthetic"
  batch_size: 256
  loader_workers: 32


